{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quang-m-nguyen/DeepPGD/blob/main/bert_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bmz33f_KotBp",
        "outputId": "4a5dd959-0194-43d0-b384-ed539679e535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.66.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy tensorflow transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0Ah7qBIWou29",
        "outputId": "cdcdcc65-f8e8-4d65-97db-f5583f0529d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KR0Os0MVotBp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
        "import os\n",
        "\n",
        "def load_data(tsv_file):\n",
        "    \"\"\"\n",
        "    Load sequences and labels from a TSV file.\n",
        "    Args:\n",
        "        tsv_file (str): Path to the TSV file.\n",
        "    Returns:\n",
        "        sequences (List[str]): List of DNA sequences.\n",
        "        labels (List[int]): List of labels (0 or 1).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(tsv_file, sep='\\t')\n",
        "    sequences = df['text'].tolist()\n",
        "    labels = df['label'].tolist()\n",
        "    return sequences, labels\n",
        "\n",
        "def tokenize_sequences(sequences, tokenizer, max_length=64):\n",
        "    \"\"\"\n",
        "    Tokenize DNA sequences using DNABERT2 tokenizer.\n",
        "    Args:\n",
        "        sequences (List[str]): List of DNA sequences.\n",
        "        tokenizer: Tokenizer object.\n",
        "        max_length (int): Maximum sequence length.\n",
        "    Returns:\n",
        "        input_ids, attention_mask\n",
        "    \"\"\"\n",
        "    encodings = tokenizer(\n",
        "        sequences,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return encodings['input_ids'], encodings['attention_mask']\n",
        "\n",
        "def build_dnabert_classification_model(pretrained_model_name_or_path, max_length=64):\n",
        "    \"\"\"\n",
        "    Build a DNABERT2 model for binary classification.\n",
        "    Args:\n",
        "        pretrained_model_name_or_path (str): Name or path of the pretrained DNABERT2 model.\n",
        "        max_length (int): Maximum sequence length.\n",
        "    Returns:\n",
        "        model (TFBertForSequenceClassification): A compiled Keras model ready for training.\n",
        "    \"\"\"\n",
        "    # Load DNABERT2 model\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        num_labels=1,\n",
        "        from_pt=True\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_dnabert_finetuning_model(\n",
        "    train_tsv,\n",
        "    val_tsv,\n",
        "    pretrained_model_name_or_path='zhihan1996/DNABERT-2-117M',\n",
        "    max_length=64,\n",
        "    epochs=3,\n",
        "    batch_size=16\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tune DNABERT2 for binary classification.\n",
        "    Args:\n",
        "        train_tsv (str): Path to the training TSV file.\n",
        "        val_tsv (str): Path to the validation TSV file.\n",
        "        pretrained_model_name_or_path (str): DNABERT2 model identifier.\n",
        "        max_length (int): Maximum sequence length for tokenization.\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Training batch size.\n",
        "    Returns:\n",
        "        model (TFBertForSequenceClassification): The trained model.\n",
        "    \"\"\"\n",
        "    # Check if files exist\n",
        "    if not os.path.isfile(train_tsv):\n",
        "        raise FileNotFoundError(f\"Training file not found: {train_tsv}\")\n",
        "    if not os.path.isfile(val_tsv):\n",
        "        raise FileNotFoundError(f\"Validation file not found: {val_tsv}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Load and tokenize training data\n",
        "    X_train_sequences, y_train = load_data(train_tsv)\n",
        "    X_train_input_ids, X_train_attention_mask = tokenize_sequences(\n",
        "        X_train_sequences, tokenizer, max_length\n",
        "    )\n",
        "    y_train = np.array(y_train).astype('float32')\n",
        "\n",
        "    # Load and tokenize validation data\n",
        "    X_val_sequences, y_val = load_data(val_tsv)\n",
        "    X_val_input_ids, X_val_attention_mask = tokenize_sequences(\n",
        "        X_val_sequences, tokenizer, max_length\n",
        "    )\n",
        "    y_val = np.array(y_val).astype('float32')\n",
        "\n",
        "    # Build the model\n",
        "    model = build_dnabert_classification_model(\n",
        "        pretrained_model_name_or_path, max_length\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x={\n",
        "            'input_ids': X_train_input_ids,\n",
        "            'attention_mask': X_train_attention_mask\n",
        "        },\n",
        "        y=y_train,\n",
        "        validation_data=(\n",
        "            {\n",
        "                'input_ids': X_val_input_ids,\n",
        "                'attention_mask': X_val_attention_mask\n",
        "            },\n",
        "            y_val\n",
        "        ),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        use_multiprocessing=True,\n",
        "        workers=8\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CQnXP0TotBq",
        "outputId": "fb6f61aa-074a-435d-9e2d-05e5fce96411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.encoder.layer.3.mlp.wo.weight', 'bert.encoder.layer.9.attention.self.Wqkv.bias', 'bert.encoder.layer.0.mlp.wo.weight', 'bert.encoder.layer.2.mlp.layernorm.weight', 'bert.encoder.layer.4.mlp.layernorm.bias', 'bert.encoder.layer.5.mlp.layernorm.weight', 'bert.encoder.layer.4.attention.self.Wqkv.weight', 'bert.encoder.layer.7.attention.self.Wqkv.bias', 'bert.encoder.layer.0.mlp.gated_layers.weight', 'bert.encoder.layer.6.attention.self.Wqkv.weight', 'bert.encoder.layer.6.mlp.layernorm.bias', 'bert.encoder.layer.4.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.bias', 'bert.encoder.layer.1.mlp.layernorm.weight', 'bert.encoder.layer.10.mlp.layernorm.bias', 'bert.encoder.layer.11.attention.self.Wqkv.weight', 'bert.encoder.layer.3.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.wo.bias', 'bert.encoder.layer.8.attention.self.Wqkv.bias', 'bert.encoder.layer.8.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.wo.weight', 'bert.encoder.layer.11.mlp.layernorm.bias', 'bert.encoder.layer.0.attention.self.Wqkv.bias', 'bert.encoder.layer.2.attention.self.Wqkv.bias', 'bert.encoder.layer.1.attention.self.Wqkv.bias', 'bert.encoder.layer.8.mlp.wo.weight', 'bert.encoder.layer.6.mlp.wo.weight', 'bert.encoder.layer.5.mlp.gated_layers.weight', 'bert.encoder.layer.7.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.wo.bias', 'bert.encoder.layer.6.mlp.gated_layers.weight', 'bert.encoder.layer.11.mlp.layernorm.weight', 'bert.encoder.layer.0.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.wo.bias', 'bert.encoder.layer.3.attention.self.Wqkv.weight', 'bert.encoder.layer.11.mlp.gated_layers.weight', 'bert.encoder.layer.5.attention.self.Wqkv.bias', 'bert.encoder.layer.11.mlp.wo.bias', 'bert.encoder.layer.1.attention.self.Wqkv.weight', 'bert.encoder.layer.0.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.gated_layers.weight', 'bert.encoder.layer.4.attention.self.Wqkv.bias', 'bert.encoder.layer.4.mlp.wo.weight', 'bert.encoder.layer.1.mlp.gated_layers.weight', 'bert.encoder.layer.10.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.wo.bias', 'bert.encoder.layer.8.mlp.layernorm.bias', 'bert.encoder.layer.2.mlp.layernorm.bias', 'bert.encoder.layer.7.mlp.layernorm.weight', 'bert.encoder.layer.5.mlp.layernorm.bias', 'bert.encoder.layer.0.mlp.layernorm.weight', 'bert.encoder.layer.3.mlp.layernorm.weight', 'bert.encoder.layer.1.mlp.layernorm.bias', 'bert.encoder.layer.2.attention.self.Wqkv.weight', 'bert.encoder.layer.7.attention.self.Wqkv.weight', 'bert.encoder.layer.9.attention.self.Wqkv.weight', 'bert.encoder.layer.6.attention.self.Wqkv.bias', 'bert.encoder.layer.7.mlp.wo.weight', 'bert.encoder.layer.11.mlp.wo.weight', 'bert.encoder.layer.1.mlp.wo.weight', 'bert.encoder.layer.9.mlp.layernorm.bias', 'bert.encoder.layer.10.mlp.wo.weight', 'bert.encoder.layer.2.mlp.gated_layers.weight', 'bert.encoder.layer.9.mlp.wo.bias', 'bert.encoder.layer.3.attention.self.Wqkv.bias', 'bert.encoder.layer.2.mlp.wo.weight', 'bert.encoder.layer.8.mlp.gated_layers.weight', 'bert.encoder.layer.5.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.layernorm.weight', 'bert.encoder.layer.10.attention.self.Wqkv.weight', 'bert.encoder.layer.9.mlp.gated_layers.weight', 'bert.encoder.layer.6.mlp.wo.bias', 'bert.encoder.layer.4.mlp.gated_layers.weight', 'bert.encoder.layer.10.attention.self.Wqkv.bias', 'bert.encoder.layer.11.attention.self.Wqkv.bias', 'bert.encoder.layer.2.mlp.wo.bias', 'bert.encoder.layer.5.mlp.wo.weight', 'bert.encoder.layer.4.mlp.layernorm.weight', 'bert.encoder.layer.8.mlp.wo.bias', 'bert.encoder.layer.3.mlp.wo.bias', 'bert.encoder.layer.8.mlp.layernorm.weight']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 56/247 [=====>........................] - ETA: 9:57 - loss: 0.6701 - accuracy: 0.5497 "
          ]
        }
      ],
      "source": [
        "train_tsv = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train.tsv'  # Update this path\n",
        "val_tsv = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test.tsv'  # Update this path\n",
        "model = train_dnabert_finetuning_model(\n",
        "    train_tsv,\n",
        "    val_tsv,\n",
        "    pretrained_model_name_or_path='zhihan1996/DNABERT-2-117M',\n",
        "    max_length=64,\n",
        "    epochs=3,\n",
        "    batch_size=64\n",
        ")\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/deepPGD/dnabert_finetuned_model')\n",
        "\n",
        "\n",
        "print(\"Model training completed and saved.\")\n",
        "\n",
        "# Optional: Evaluate the model on the validation set\n",
        "X_val_sequences, y_val = load_data(val_tsv)\n",
        "tokenizer = AutoTokenizer.from_pretrained('zhihan1996/DNABERT-2-117M', trust_remote_code=True)\n",
        "X_val_input_ids, X_val_attention_mask = tokenize_sequences(X_val_sequences, tokenizer, max_length=64)\n",
        "y_val = np.array(y_val).astype('float32')\n",
        "\n",
        "evaluation = model.evaluate(\n",
        "    x={'input_ids': X_val_input_ids, 'attention_mask': X_val_attention_mask},\n",
        "    y=y_val\n",
        ")\n",
        "print(f\"Validation Loss: {evaluation[0]:.4f}\")\n",
        "print(f\"Validation Accuracy: {evaluation[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzoZk_wGpzI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}