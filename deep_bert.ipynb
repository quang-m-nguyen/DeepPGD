{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quang-m-nguyen/DeepPGD/blob/main/deep_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ImeIY8CbpXP",
        "outputId": "889a1c41-2bcb-48ec-e200-21bc0666a049"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "24WMAyCfLp_2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "# from keras_self_attention import SeqSelfAttention\n",
        "# from keras.utils import to_categorical\n",
        "from keras.layers import (\n",
        "    Bidirectional,\n",
        "    Conv1D,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    Input,\n",
        "    LayerNormalization,\n",
        "    concatenate,\n",
        "    LSTM\n",
        ")\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
        "from tensorflow.keras import initializers, layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y_train():\n",
        "    train_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_C.equisetifolia/train.tsv'\n",
        "    test_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test.tsv'\n",
        "\n",
        "    x_test = np.array([])\n",
        "    y_test = np.array([])\n",
        "    x_train = np.array([])\n",
        "    y_train = np.array([])\n",
        "\n",
        "    test_labels = []\n",
        "    three_er_list = []\n",
        "\n",
        "\n",
        "    train_data = pd.read_csv(train_filename,header = None, sep = \"\\t\")\n",
        "    test_data  = pd.read_csv(test_filename,header = None, sep = \"\\t\")\n",
        "\n",
        "    # 0,1\n",
        "    y_train =  train_data[1][:]\n",
        "\n",
        "    y_test = test_data[1][:]\n",
        "\n",
        "    return y_train.to_numpy()  , y_test.to_numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "8h-W-J1gK83b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_test_labels(y_data):\n",
        "  test_labels = []\n",
        "  for i in range(1, len(y_data)):\n",
        "    if(y_data[i] == \"1\"):\n",
        "        test_labels.append([1,0])\n",
        "    elif(y_data[i] == \"0\"):\n",
        "        test_labels.append([0,1])\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "  return test_labels"
      ],
      "metadata": {
        "id": "jhjVafuUOYzI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def get_embeddings_array(pkl_file_path):\n",
        "    \"\"\"\n",
        "    Reads a pickle file containing embeddings data and returns the embeddings as a NumPy array.\n",
        "\n",
        "    Args:\n",
        "        pkl_file_path (str): Path to the pickle file.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array containing the embeddings.\n",
        "    \"\"\"\n",
        "    with open(pkl_file_path, 'rb') as f:\n",
        "        embeddings_data = pickle.load(f)\n",
        "\n",
        "    embeddings_list = [item['embedding'].detach().numpy() for item in embeddings_data] # Detach the tensor from the computation graph\n",
        "    return np.array(embeddings_list)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PtSEWOkJoFQM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data_v1():\n",
        "  # Example usage\n",
        "  pkl_file_path = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train_embeddings_data.pkl'\n",
        "  x_train = get_embeddings_array(pkl_file_path)\n",
        "\n",
        "  print('prepare_training_data_v1: x_train')\n",
        "  print(len(x_train))\n",
        "\n",
        "  pkl_file_path = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test_embeddings_data.pkl'\n",
        "  x_test = get_embeddings_array(pkl_file_path)\n",
        "\n",
        "  y_train, y_test = get_y_train()\n",
        "\n",
        "  x_train_len = len(x_train)\n",
        "\n",
        "  x_data = np.concatenate((x_train, x_test), axis=0)\n",
        "  y_data = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "  test_labels = make_test_labels(y_data)\n",
        "\n",
        "  x_train,x_test = x_data[:x_train_len],x_data[x_train_len:]\n",
        "\n",
        "  y_train,y_test = test_labels[:x_train_len],test_labels[x_train_len:]\n",
        "\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "yVkMaQOrP32L"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data():\n",
        "    train_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train.tsv'\n",
        "    test_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test.tsv'\n",
        "\n",
        "    x_test = np.array([])\n",
        "    y_test = np.array([])\n",
        "    x_train = np.array([])\n",
        "    y_train = np.array([])\n",
        "\n",
        "    test_labels = []\n",
        "    three_er_list = []\n",
        "\n",
        "    K_MER = 3\n",
        "\n",
        "\n",
        "    train_data = pd.read_csv(train_filename,header = None, sep = \"\\t\")\n",
        "    test_data  = pd.read_csv(test_filename,header = None, sep = \"\\t\")\n",
        "\n",
        "    # ACTG\n",
        "    pro_x_train = train_data[2][1:]\n",
        "    # 0,1\n",
        "    y_train =  train_data[1][:]\n",
        "    print(y_train)\n",
        "\n",
        "    pos_train_len = len(pro_x_train)\n",
        "\n",
        "    pro_x_test =  test_data[2][1:]\n",
        "    y_test = test_data[1][:]\n",
        "\n",
        "    pro_x_data  = pd.concat([pro_x_train,pro_x_test],ignore_index= True )\n",
        "    pro_y_data  = pd.concat([y_train,y_test],ignore_index= True )\n",
        "\n",
        "\n",
        "    for i in range(1, len(pro_y_data)):\n",
        "      if(pro_y_data[i] == \"1\"):\n",
        "          test_labels.append([1,0])\n",
        "      elif(pro_y_data[i] == \"0\"):\n",
        "          test_labels.append([0,1])\n",
        "      else:\n",
        "          continue\n",
        "\n",
        "    # K-mer Encoding for DNA Sequences\n",
        "    #\n",
        "    # Purpose:\n",
        "    # - Transform variable-length DNA sequences into fixed-length feature representations\n",
        "    # - Capture local sequence patterns that may be relevant to DNA methylation sites\n",
        "    # - Create a suitable input format for machine learning models\n",
        "    #\n",
        "    # Functionality:\n",
        "    # 1. Set k-mer size (K=3 in this case)\n",
        "    # 2. For each DNA sequence:\n",
        "    #    a. Convert to string and remove any extra characters\n",
        "    #    b. Generate all possible k-mers (substrings of length K)\n",
        "    #    c. Store k-mers for each sequence in a list\n",
        "    # 3. Collect k-mer lists for all sequences in str_array\n",
        "    #\n",
        "    # Benefits:\n",
        "    # - Captures local sequence context\n",
        "    # - Provides fixed-length representation for variable-length sequences\n",
        "    # - Reduces sequence complexity while retaining important features\n",
        "    # - Facilitates efficient sequence comparison and analysis\n",
        "    # - Improves feature extraction for machine learning models\n",
        "    #\n",
        "    # Example:\n",
        "    # Input DNA sequence: \"ATCGATCG\"\n",
        "    # Resulting k-mers (K=3): [\"ATC\", \"TCG\", \"CGA\", \"GAT\", \"ATC\", \"TCG\"]\n",
        "    #\n",
        "    # Data structure:\n",
        "    # str_array = [\n",
        "    #     [\"ATC\", \"TCG\", \"CGA\", \"GAT\", \"ATC\", \"TCG\"],\n",
        "    # ]\n",
        "\n",
        "    for i in pro_x_data:\n",
        "        seq_str = str(i)\n",
        "        seq_str = seq_str.strip('[]\\'')\n",
        "        t=0\n",
        "        l=[]\n",
        "        for index in range(len(seq_str)):\n",
        "            t=seq_str[index:index+K_MER]\n",
        "            if (len(t))==K_MER:\n",
        "                l.append(t)\n",
        "        three_er_list.append(l)\n",
        "\n",
        "\n",
        "\n",
        "    # DNA Sequence Preprocessing\n",
        "    # Purpose: Turn DNA chunks into number lists for machine learning\n",
        "    # Steps:\n",
        "    # 1. Assign a unique number to each DNA chunk (up to 30,000 most common chunks)\n",
        "    # 2. Convert each DNA sequence to a list of these numbers tokens\n",
        "    # 3. Make all lists the same length (48) by adding zeros at the end if needed\n",
        "\n",
        "    # Example:\n",
        "    # Input DNA sequences:\n",
        "    #   [\"ATCG\", \"CGTA\", \"ATCGATCG\"]\n",
        "    #\n",
        "    # After assigning numbers:\n",
        "    #   ATCG -> 1, CGTA -> 2, GATC -> 3\n",
        "    #\n",
        "    # Converted to number lists:\n",
        "    #   [1, 2]\n",
        "    #   [2, 1]\n",
        "    #   [1, 3, 1]\n",
        "    #\n",
        "    # Final output (padded to length 48):\n",
        "    #   [1, 2, 0, 0, ..., 0]  (46 zeros)\n",
        "    #   [2, 1, 0, 0, ..., 0]  (46 zeros)\n",
        "    #   [1, 3, 1, 0, ..., 0]  (45 zeros)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words = 30000)\n",
        "    tokenizer.fit_on_texts(three_er_list)\n",
        "    sequences = tokenizer.texts_to_sequences(three_er_list)\n",
        "    sequences = pad_sequences(sequences, maxlen = 48, padding = \"post\")\n",
        "    sequences = np.array(sequences)\n",
        "\n",
        "\n",
        "    x_train,x_test = sequences[:pos_train_len],sequences[pos_train_len:]\n",
        "    # print(x_train)\n",
        "\n",
        "    y_train,y_test = test_labels[:pos_train_len],test_labels[pos_train_len:]\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3gJ9iRYWcIGI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train, x_test, y_train, y_test = prepare_training_data()\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "\n",
        "x_train, x_test, y_train, y_test = prepare_training_data_v1()\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "# print(y_train.shape)\n",
        "# print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKENK8rug-4E",
        "outputId": "5905941f-584c-43e7-b4a0-47a3bf3af13f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(io.BytesIO(b))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prepare_training_data_v1: x_train\n",
            "50\n",
            "(50, 768)\n",
            "(50, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masked_data(x_train, mask_percentage):\n",
        "    \"\"\"\n",
        "    Create a masked version of the input data.\n",
        "\n",
        "    Args:\n",
        "    x_train (numpy.ndarray): Input data to be masked.\n",
        "    mask_percentage (float): Percentage of data to be masked, between 0 and 1.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Masked version of the input data.\n",
        "    \"\"\"\n",
        "    # Validate mask_percentage range\n",
        "    if mask_percentage < 0.0 or mask_percentage > 1.0:\n",
        "        raise ValueError(\"mask_percentage must be between 0 and 1.\")\n",
        "\n",
        "    # Create a boolean mask: True with probability mask_percentage\n",
        "    mask = np.random.random(x_train.shape) < mask_percentage\n",
        "\n",
        "    # Apply the mask: keep original values where mask is False, set to 0 where mask is True\n",
        "    masked_data = np.where(mask, 0.0, x_train)\n",
        "\n",
        "    return np.array(masked_data)"
      ],
      "metadata": {
        "id": "QX8EMxXOq3Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_architecture():\n",
        "    t = time.time()\n",
        "    my_time = int(round(t * 1000)) % 2147483648\n",
        "    np.random.seed(my_time)\n",
        "\n",
        "    sequence_input = Input(shape=(48,))\n",
        "    embedding_layer = Embedding(30000, 16)\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "\n",
        "    stem = LayerNormalization(epsilon=1e-6)(embedded_sequences)\n",
        "    stem = Dropout(0.2)(stem)\n",
        "\n",
        "    MLP_1 = Dense(\n",
        "        int(16 * 2.0),\n",
        "        name=\"Dense_0\",\n",
        "        kernel_initializer=initializers.GlorotUniform(),\n",
        "        bias_initializer=initializers.RandomNormal(stddev=1e-6),\n",
        "    )(stem)\n",
        "    Activation_1 = layers.Activation(\"gelu\")(MLP_1)\n",
        "    Activation_1 = LayerNormalization(epsilon=1e-6)(Activation_1)\n",
        "    Dropout_2 = Dropout(0.2)(Activation_1)\n",
        "\n",
        "    MLP_2 = Dense(\n",
        "        int(16 * 1.0),\n",
        "        name=\"Dense_1\",\n",
        "        kernel_initializer=initializers.GlorotUniform(),\n",
        "        bias_initializer=initializers.RandomNormal(stddev=1e-6),\n",
        "    )(Dropout_2)\n",
        "    MLP_2 = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=16)(MLP_2, stem)\n",
        "    stem = stem + MLP_2\n",
        "    stem = Dropout(0.2)(stem)\n",
        "\n",
        "    lstm = Bidirectional(LSTM(16, return_sequences=True))(stem)\n",
        "    lstm = layers.Activation(\"gelu\")(lstm)\n",
        "    lstm = LayerNormalization(epsilon=1e-6)(lstm)\n",
        "    lstm = Dropout(0.5)(lstm)\n",
        "\n",
        "    lstm1 = Bidirectional(LSTM(16, return_sequences=True))(lstm)\n",
        "    lstm1 = layers.Activation(\"gelu\")(lstm1)\n",
        "    lstm1 = LayerNormalization(epsilon=1e-6)(lstm1)\n",
        "    lstm1 = Dropout(0.2)(lstm1)\n",
        "    lstm1 = lstm1 + lstm\n",
        "\n",
        "    lstm2 = Bidirectional(LSTM(16, return_sequences=True))(lstm1)\n",
        "    lstm2 = layers.Activation(\"gelu\")(lstm2)\n",
        "    lstm2 = LayerNormalization(epsilon=1e-6)(lstm2)\n",
        "    lstm2 = Dropout(0.2)(lstm2)\n",
        "\n",
        "    lstm2 = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=16)(lstm2, lstm)\n",
        "    lstm2 = lstm2 + lstm\n",
        "\n",
        "    cnn1_24 = Conv1D(\n",
        "        filters=32, kernel_size=9, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(stem)\n",
        "    cnn1_16 = Conv1D(\n",
        "        filters=64, kernel_size=8, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(stem)\n",
        "    cnn1_10 = Conv1D(\n",
        "        filters=32, kernel_size=5, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(stem)\n",
        "    merge_fla_1 = concatenate([cnn1_24, cnn1_16, cnn1_10], axis=2)\n",
        "    merge_fla_1 = LayerNormalization(epsilon=1e-6)(merge_fla_1)\n",
        "    merge_fla_1 = Dropout(0.2)(merge_fla_1)\n",
        "\n",
        "    cnn2_24 = Conv1D(\n",
        "        filters=32, kernel_size=9, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_1)\n",
        "    cnn2_16 = Conv1D(\n",
        "        filters=64, kernel_size=8, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_1)\n",
        "    cnn2_10 = Conv1D(\n",
        "        filters=32, kernel_size=5, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_1)\n",
        "\n",
        "    merge_fla_2 = concatenate([cnn2_24, cnn2_16, cnn2_10], axis=2)\n",
        "    merge_fla_2 = merge_fla_2 + merge_fla_1\n",
        "    merge_fla_2 = LayerNormalization(epsilon=1e-6)(merge_fla_2)\n",
        "    merge_fla_2 = Dropout(0.2)(merge_fla_2)\n",
        "\n",
        "    cnn3_24 = Conv1D(\n",
        "        filters=32, kernel_size=9, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_2)\n",
        "    cnn3_16 = Conv1D(\n",
        "        filters=64, kernel_size=8, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_2)\n",
        "    cnn3_10 = Conv1D(\n",
        "        filters=32, kernel_size=5, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_2)\n",
        "\n",
        "    merge_fla_3 = concatenate([cnn3_24, cnn3_16, cnn3_10], axis=2)\n",
        "    merge_fla_3 = merge_fla_2 + merge_fla_3\n",
        "    merge_fla_3 = LayerNormalization(epsilon=1e-6)(merge_fla_3)\n",
        "    merge_fla_3 = Dropout(0.2)(merge_fla_3)\n",
        "\n",
        "    cnn4_24 = Conv1D(\n",
        "        filters=32, kernel_size=9, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_3)\n",
        "    cnn4_16 = Conv1D(\n",
        "        filters=64, kernel_size=8, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_3)\n",
        "    cnn4_10 = Conv1D(\n",
        "        filters=32, kernel_size=5, strides=1, activation=\"gelu\", padding=\"causal\"\n",
        "    )(merge_fla_3)\n",
        "\n",
        "    merge_fla_4 = concatenate([cnn4_24, cnn4_16, cnn4_10], axis=2)\n",
        "    merge_fla_4 = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=16)(\n",
        "        merge_fla_1, merge_fla_4\n",
        "    )\n",
        "    merge_fla_4 = merge_fla_1 + merge_fla_4\n",
        "    merge_fla_4 = LayerNormalization(epsilon=1e-6)(merge_fla_4)\n",
        "    merge_fla_4 = Dropout(0.2)(merge_fla_4)\n",
        "    lstm = concatenate([merge_fla_4, lstm2])\n",
        "\n",
        "    lstm = Flatten()(lstm)\n",
        "    # merge_fla = Dropout(0.2)(lstm)\n",
        "\n",
        "    merge = Dense(200, activation=\"sigmoid\")(lstm)\n",
        "    merge = Dropout(0.5)(merge)\n",
        "\n",
        "    preds = Dense(2, activation=\"softmax\")(merge)\n",
        "    model = Model(sequence_input, preds)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "poIBSw3lgNKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AUCMCCCallback(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.last_epoch_logs = {}\n",
        "        self.best_val_accuracy = 0.0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.last_epoch_logs = logs.copy()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        if epoch < 1:\n",
        "            return\n",
        "\n",
        "        x_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(x_val)\n",
        "\n",
        "        # Calculate metrics using TensorFlow operations\n",
        "        auc = roc_auc_score(y_val, y_pred)\n",
        "\n",
        "        # Convert predictions and labels to binary for MCC\n",
        "        y_pred_binary = tf.argmax(y_pred, axis=1)\n",
        "        y_val_binary = tf.argmax(y_val, axis=1)\n",
        "        mcc = matthews_corrcoef(y_val_binary.numpy(), y_pred_binary.numpy())  # MCC needs NumPy arrays\n",
        "\n",
        "        # Calculate accuracy with TensorFlow\n",
        "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_binary, y_val_binary), tf.float32))\n",
        "\n",
        "        print(f\"\\nValidation AUC: {auc:.4f} - MCC: {mcc:.4f} - ACC: {accuracy.numpy():.4f}\")\n",
        "\n",
        "        # Track and print best validation accuracy\n",
        "        val_accuracy = self.last_epoch_logs.get(\"val_accuracy\", 0)\n",
        "        if val_accuracy > self.best_val_accuracy:\n",
        "            self.best_val_accuracy = val_accuracy\n",
        "        print(\"epoch[\", epoch, \"].val_accuracy:\", val_accuracy)\n",
        "        print(\"epoch[\", epoch, \"].best_accuracy:\", self.best_val_accuracy)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cg2xTk43O5i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "x_train, x_test, y_train, y_test = prepare_training_data()\n",
        "\n",
        "# Convert NumPy arrays to TensorFlow Tensors\n",
        "x_train = tf.convert_to_tensor(x_train)\n",
        "x_test = tf.convert_to_tensor(x_test)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "y_test = tf.convert_to_tensor(y_test)\n",
        "# print(x_train)\n",
        "mask_percentage = 0.15  # Mask 15% of the data\n",
        "x_train = create_masked_data(x_train, mask_percentage)\n",
        "model = make_model_architecture(x_train)\n",
        "\n",
        "auc_mcc_callback = AUCMCCCallback(validation_data=(x_test, y_test))\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=30,\n",
        "    epochs=100,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[auc_mcc_callback],\n",
        ")\n",
        "\n",
        "# accuracy_best_list[number] = best_acc\n",
        "\n",
        "# for i in accuracy_best_list.keys():\n",
        "#     print(\"best_acc[\", i, \"] = \", accuracy_best_list[i])\n",
        "# avg = float(sum(accuracy_best_list.values())) / len(accuracy_best_list)\n",
        "# print()\n",
        "# print(\"best_acc[avg] = \", avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "5Ooo1TL6PhaW",
        "outputId": "90de0673-b849-445d-8bf7-285f5de60e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'return' outside function (<ipython-input-7-0276b0ea337b>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0276b0ea337b>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Draw architecture\n",
        "\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# plot_model(\n",
        "#     model,\n",
        "#     to_file='model_plot.png',         # Save the plot to a file\n",
        "#     show_shapes=True,                 # Show input and output shapes\n",
        "#     show_layer_names=True,            # Show layer names\n",
        "#     dpi=96,                           # Set the resolution of the plot\n",
        "# )\n"
      ],
      "metadata": {
        "id": "FAXPEpY9utUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def get_pkl_length(pkl_file_path):\n",
        "    \"\"\"\n",
        "    Opens a pickle file, loads the data, and returns the length of the loaded object.\n",
        "\n",
        "    Args:\n",
        "        pkl_file_path (str): Path to the pickle file.\n",
        "\n",
        "    Returns:\n",
        "        int: Length of the loaded object.\n",
        "    \"\"\"\n",
        "    with open(pkl_file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return len(data)\n",
        "\n",
        "# Example usage:\n",
        "pkl_file_path = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test_embeddings_data.pkl'\n",
        "length = get_pkl_length(pkl_file_path)\n",
        "print(f\"Length of the data in the pickle file: {length}\")"
      ],
      "metadata": {
        "id": "Vl9Y8y7Z8KBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33ea27e-7b99-45df-ca7f-bcafd4f064f0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the data in the pickle file: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_tsv_length(tsv_file_path):\n",
        "    \"\"\"\n",
        "    Reads a TSV file and returns the number of rows (length) in the file.\n",
        "\n",
        "    Args:\n",
        "        tsv_file_path (str): Path to the TSV file.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of rows in the TSV file.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(tsv_file_path, sep='\\t', header=None)  # Read TSV file into a DataFrame\n",
        "    return len(df)  # Return the number of rows\n",
        "\n",
        "# Example usage:\n",
        "tsv_file_path = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train.tsv'\n",
        "length = get_tsv_length(tsv_file_path)\n",
        "print(f\"Number of rows in the TSV file: {length}\")"
      ],
      "metadata": {
        "id": "LREyq_Zxgn2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f08e21-e9a7-4d80-ad3d-320bbd50203b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the TSV file: 15799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5zCrvEOyS5BH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}