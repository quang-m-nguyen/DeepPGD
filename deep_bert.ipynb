{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quang-m-nguyen/DeepPGD/blob/main/deep_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1ImeIY8CbpXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24WMAyCfLp_2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "# from keras_self_attention import SeqSelfAttention\n",
        "# from keras.utils import to_categorical\n",
        "from keras.layers import (\n",
        "    Bidirectional,\n",
        "    Conv1D,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    Input,\n",
        "    LayerNormalization,\n",
        "    concatenate,\n",
        "    LSTM\n",
        ")\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
        "from tensorflow.keras import initializers, layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y_train():\n",
        "    train_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train.tsv'\n",
        "    test_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test.tsv'\n",
        "\n",
        "    x_test = np.array([])\n",
        "    y_test = np.array([])\n",
        "    x_train = np.array([])\n",
        "    y_train = np.array([])\n",
        "\n",
        "    test_labels = []\n",
        "    three_er_list = []\n",
        "\n",
        "\n",
        "    train_data = pd.read_csv(train_filename,header = None, sep = \"\\t\")\n",
        "    test_data  = pd.read_csv(test_filename,header = None, sep = \"\\t\")\n",
        "\n",
        "    # 0,1\n",
        "    y_train =  train_data[1][:]\n",
        "\n",
        "    y_test = test_data[1][:]\n",
        "\n",
        "    return y_train.to_numpy()  , y_test.to_numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "8h-W-J1gK83b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_test_labels(y_data):\n",
        "  test_labels = []\n",
        "  for i in range(1, len(y_data)):\n",
        "    if(y_data[i] == \"1\"):\n",
        "        test_labels.append([1,0])\n",
        "    elif(y_data[i] == \"0\"):\n",
        "        test_labels.append([0,1])\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "  return test_labels"
      ],
      "metadata": {
        "id": "jhjVafuUOYzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def get_embeddings_array(pkl_file_path):\n",
        "    \"\"\"\n",
        "    Reads a pickle file containing embeddings data and returns the embeddings as a NumPy array.\n",
        "\n",
        "    Args:\n",
        "        pkl_file_path (str): Path to the pickle file.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array containing the embeddings.\n",
        "    \"\"\"\n",
        "    with open(pkl_file_path, 'rb') as f:\n",
        "        embeddings_data = pickle.load(f)\n",
        "\n",
        "    embeddings_list = [item['embedding'].detach().numpy() for item in embeddings_data] # Detach the tensor from the computation graph\n",
        "    return np.array(embeddings_list)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PtSEWOkJoFQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def prepare_training_data_v1():\n",
        "    # Example usage\n",
        "    pkl_file_path_train = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train_embeddings_data.pkl'\n",
        "    x_train = get_embeddings_array(pkl_file_path_train)\n",
        "\n",
        "    print('prepare_training_data_v2: x_train')\n",
        "    print(len(x_train))\n",
        "\n",
        "    pkl_file_path_test = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test_embeddings_data.pkl'\n",
        "    x_test = get_embeddings_array(pkl_file_path_test)\n",
        "\n",
        "    y_train, y_test = get_y_train()\n",
        "\n",
        "    # Concatenate train and test data\n",
        "    x_data = np.concatenate((x_train, x_test), axis=0)\n",
        "    y_data = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "    # Create test labels from y_data\n",
        "    test_labels = make_test_labels(y_data)\n",
        "\n",
        "    # Split the data into train and test sets with stratified sampling\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x_data, test_labels, test_size=0.10, stratify=test_labels\n",
        "    )\n",
        "\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "yVkMaQOrP32L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data():\n",
        "    train_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train.tsv'\n",
        "    test_filename = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test.tsv'\n",
        "\n",
        "    x_test = np.array([])\n",
        "    y_test = np.array([])\n",
        "    x_train = np.array([])\n",
        "    y_train = np.array([])\n",
        "\n",
        "    test_labels = []\n",
        "    three_er_list = []\n",
        "\n",
        "    K_MER = 3\n",
        "\n",
        "\n",
        "    train_data = pd.read_csv(train_filename,header = None, sep = \"\\t\")\n",
        "    test_data  = pd.read_csv(test_filename,header = None, sep = \"\\t\")\n",
        "\n",
        "    # ACTG\n",
        "    pro_x_train = train_data[2][1:]\n",
        "    # 0,1\n",
        "    y_train =  train_data[1][:]\n",
        "    print(y_train)\n",
        "\n",
        "    pos_train_len = len(pro_x_train)\n",
        "\n",
        "    pro_x_test =  test_data[2][1:]\n",
        "    y_test = test_data[1][:]\n",
        "\n",
        "    pro_x_data  = pd.concat([pro_x_train,pro_x_test],ignore_index= True )\n",
        "    pro_y_data  = pd.concat([y_train,y_test],ignore_index= True )\n",
        "\n",
        "\n",
        "    for i in range(1, len(pro_y_data)):\n",
        "      if(pro_y_data[i] == \"1\"):\n",
        "          test_labels.append([1,0])\n",
        "      elif(pro_y_data[i] == \"0\"):\n",
        "          test_labels.append([0,1])\n",
        "      else:\n",
        "          continue\n",
        "\n",
        "    # K-mer Encoding for DNA Sequences\n",
        "    #\n",
        "    # Purpose:\n",
        "    # - Transform variable-length DNA sequences into fixed-length feature representations\n",
        "    # - Capture local sequence patterns that may be relevant to DNA methylation sites\n",
        "    # - Create a suitable input format for machine learning models\n",
        "    #\n",
        "    # Functionality:\n",
        "    # 1. Set k-mer size (K=3 in this case)\n",
        "    # 2. For each DNA sequence:\n",
        "    #    a. Convert to string and remove any extra characters\n",
        "    #    b. Generate all possible k-mers (substrings of length K)\n",
        "    #    c. Store k-mers for each sequence in a list\n",
        "    # 3. Collect k-mer lists for all sequences in str_array\n",
        "    #\n",
        "    # Benefits:\n",
        "    # - Captures local sequence context\n",
        "    # - Provides fixed-length representation for variable-length sequences\n",
        "    # - Reduces sequence complexity while retaining important features\n",
        "    # - Facilitates efficient sequence comparison and analysis\n",
        "    # - Improves feature extraction for machine learning models\n",
        "    #\n",
        "    # Example:\n",
        "    # Input DNA sequence: \"ATCGATCG\"\n",
        "    # Resulting k-mers (K=3): [\"ATC\", \"TCG\", \"CGA\", \"GAT\", \"ATC\", \"TCG\"]\n",
        "    #\n",
        "    # Data structure:\n",
        "    # str_array = [\n",
        "    #     [\"ATC\", \"TCG\", \"CGA\", \"GAT\", \"ATC\", \"TCG\"],\n",
        "    # ]\n",
        "\n",
        "    for i in pro_x_data:\n",
        "        seq_str = str(i)\n",
        "        seq_str = seq_str.strip('[]\\'')\n",
        "        t=0\n",
        "        l=[]\n",
        "        for index in range(len(seq_str)):\n",
        "            t=seq_str[index:index+K_MER]\n",
        "            if (len(t))==K_MER:\n",
        "                l.append(t)\n",
        "        three_er_list.append(l)\n",
        "\n",
        "\n",
        "\n",
        "    # DNA Sequence Preprocessing\n",
        "    # Purpose: Turn DNA chunks into number lists for machine learning\n",
        "    # Steps:\n",
        "    # 1. Assign a unique number to each DNA chunk (up to 30,000 most common chunks)\n",
        "    # 2. Convert each DNA sequence to a list of these numbers tokens\n",
        "    # 3. Make all lists the same length (48) by adding zeros at the end if needed\n",
        "\n",
        "    # Example:\n",
        "    # Input DNA sequences:\n",
        "    #   [\"ATCG\", \"CGTA\", \"ATCGATCG\"]\n",
        "    #\n",
        "    # After assigning numbers:\n",
        "    #   ATCG -> 1, CGTA -> 2, GATC -> 3\n",
        "    #\n",
        "    # Converted to number lists:\n",
        "    #   [1, 2]\n",
        "    #   [2, 1]\n",
        "    #   [1, 3, 1]\n",
        "    #\n",
        "    # Final output (padded to length 48):\n",
        "    #   [1, 2, 0, 0, ..., 0]  (46 zeros)\n",
        "    #   [2, 1, 0, 0, ..., 0]  (46 zeros)\n",
        "    #   [1, 3, 1, 0, ..., 0]  (45 zeros)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words = 30000)\n",
        "    tokenizer.fit_on_texts(three_er_list)\n",
        "    sequences = tokenizer.texts_to_sequences(three_er_list)\n",
        "    sequences = pad_sequences(sequences, maxlen = 48, padding = \"post\")\n",
        "    sequences = np.array(sequences)\n",
        "\n",
        "\n",
        "    x_train,x_test = sequences[:pos_train_len],sequences[pos_train_len:]\n",
        "    # print(x_train)\n",
        "\n",
        "    y_train,y_test = test_labels[:pos_train_len],test_labels[pos_train_len:]\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3gJ9iRYWcIGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train, x_test, y_train, y_test = prepare_training_data()\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "\n",
        "# x_train, x_test, y_train, y_test = prepare_training_data_v1()\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(y_train.shape)\n",
        "# print(y_test.shape)"
      ],
      "metadata": {
        "id": "nKENK8rug-4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masked_data(x_train, mask_percentage):\n",
        "    \"\"\"\n",
        "    Create a masked version of the input data.\n",
        "\n",
        "    Args:\n",
        "    x_train (numpy.ndarray): Input data to be masked.\n",
        "    mask_percentage (float): Percentage of data to be masked, between 0 and 1.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Masked version of the input data.\n",
        "    \"\"\"\n",
        "    # Validate mask_percentage range\n",
        "    if mask_percentage < 0.0 or mask_percentage > 1.0:\n",
        "        raise ValueError(\"mask_percentage must be between 0 and 1.\")\n",
        "\n",
        "    # Create a boolean mask: True with probability mask_percentage\n",
        "    mask = np.random.random(x_train.shape) < mask_percentage\n",
        "\n",
        "    # Apply the mask: keep original values where mask is False, set to 0 where mask is True\n",
        "    masked_data = np.where(mask, 0.0, x_train)\n",
        "\n",
        "    return np.array(masked_data)"
      ],
      "metadata": {
        "id": "QX8EMxXOq3Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, LSTM, Conv1D, GlobalAveragePooling1D,\n",
        "    concatenate, Bidirectional, LayerNormalization, Reshape\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def make_model_architecture():\n",
        "    t = time.time()\n",
        "    my_time = int(round(t * 1000)) % 2147483648\n",
        "    np.random.seed(my_time)\n",
        "\n",
        "    sequence_input = Input(shape=(768,))\n",
        "\n",
        "    # Reshape input to add a time dimension\n",
        "    x = Reshape((1, 768))(sequence_input)\n",
        "\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    # Multi-head self-attention block\n",
        "    attention = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=96)(x, x)\n",
        "    x = x + attention\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = Dense(3072, activation=\"gelu\")(x)\n",
        "    ffn = Dense(768)(ffn)\n",
        "    x = x + ffn\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # LSTM layers\n",
        "    x = Bidirectional(LSTM(384, return_sequences=True))(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    x = Bidirectional(LSTM(384, return_sequences=True))(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # Convolutional layers\n",
        "    conv1 = Conv1D(filters=256, kernel_size=3, activation=\"gelu\", padding=\"same\")(x)\n",
        "    conv2 = Conv1D(filters=256, kernel_size=5, activation=\"gelu\", padding=\"same\")(x)\n",
        "    conv3 = Conv1D(filters=256, kernel_size=7, activation=\"gelu\", padding=\"same\")(x)\n",
        "    x = concatenate([conv1, conv2, conv3], axis=-1)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Final dense layers\n",
        "    x = Dense(512, activation=\"gelu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(256, activation=\"gelu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    output = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=sequence_input, outputs=output)\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "poIBSw3lgNKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AUCMCCCallback(Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.last_epoch_logs = {}\n",
        "        self.best_val_accuracy = 0.0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.last_epoch_logs = logs.copy()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        if epoch < 1:\n",
        "            return\n",
        "\n",
        "        x_val, y_val = self.validation_data\n",
        "        y_pred = self.model.predict(x_val)\n",
        "\n",
        "        # Calculate metrics using TensorFlow operations\n",
        "        auc = roc_auc_score(y_val, y_pred)\n",
        "\n",
        "        # Convert predictions and labels to binary for MCC\n",
        "        y_pred_binary = tf.argmax(y_pred, axis=1)\n",
        "        y_val_binary = tf.argmax(y_val, axis=1)\n",
        "        mcc = matthews_corrcoef(y_val_binary.numpy(), y_pred_binary.numpy())  # MCC needs NumPy arrays\n",
        "\n",
        "        # Calculate accuracy with TensorFlow\n",
        "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_binary, y_val_binary), tf.float32))\n",
        "\n",
        "        print(f\"\\nValidation AUC: {auc:.4f} - MCC: {mcc:.4f} - ACC: {accuracy.numpy():.4f}\")\n",
        "\n",
        "        # Track and print best validation accuracy\n",
        "        val_accuracy = self.last_epoch_logs.get(\"val_accuracy\", 0)\n",
        "        if val_accuracy > self.best_val_accuracy:\n",
        "            self.best_val_accuracy = val_accuracy\n",
        "        print(\"epoch[\", epoch, \"].val_accuracy:\", val_accuracy)\n",
        "        print(\"epoch[\", epoch, \"].best_accuracy:\", self.best_val_accuracy)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cg2xTk43O5i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "x_train, x_test, y_train, y_test = prepare_training_data_v1()\n",
        "\n",
        "# Convert NumPy arrays to TensorFlow Tensors\n",
        "x_train = tf.convert_to_tensor(x_train)\n",
        "x_test = tf.convert_to_tensor(x_test)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "y_test = tf.convert_to_tensor(y_test)\n",
        "# print(x_train)\n",
        "mask_percentage = 0.15  # Mask 15% of the data\n",
        "# x_train = create_masked_data(x_train, mask_percentage)\n",
        "model = make_model_architecture()\n",
        "\n",
        "auc_mcc_callback = AUCMCCCallback(validation_data=(x_test, y_test))\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=1000,\n",
        "    epochs=100,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[auc_mcc_callback],\n",
        ")\n",
        "\n",
        "# accuracy_best_list[number] = best_acc\n",
        "\n",
        "# for i in accuracy_best_list.keys():\n",
        "#     print(\"best_acc[\", i, \"] = \", accuracy_best_list[i])\n",
        "# avg = float(sum(accuracy_best_list.values())) / len(accuracy_best_list)\n",
        "# print()\n",
        "# print(\"best_acc[avg] = \", avg)"
      ],
      "metadata": {
        "id": "5Ooo1TL6PhaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Draw architecture\n",
        "\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# plot_model(\n",
        "#     model,\n",
        "#     to_file='model_plot.png',         # Save the plot to a file\n",
        "#     show_shapes=True,                 # Show input and output shapes\n",
        "#     show_layer_names=True,            # Show layer names\n",
        "#     dpi=96,                           # Set the resolution of the plot\n",
        "# )\n"
      ],
      "metadata": {
        "id": "FAXPEpY9utUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def get_pkl_length(pkl_file_path):\n",
        "    \"\"\"\n",
        "    Opens a pickle file, loads the data, and returns the length of the loaded object.\n",
        "\n",
        "    Args:\n",
        "        pkl_file_path (str): Path to the pickle file.\n",
        "\n",
        "    Returns:\n",
        "        int: Length of the loaded object.\n",
        "    \"\"\"\n",
        "    with open(pkl_file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return len(data)\n",
        "\n",
        "# Example usage:\n",
        "pkl_file_path = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train_embeddings_data.pkl'\n",
        "length = get_pkl_length(pkl_file_path)\n",
        "print(f\"Length of the data in the pickle file: {length}\")"
      ],
      "metadata": {
        "id": "Vl9Y8y7Z8KBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_tsv_length(tsv_file_path):\n",
        "    \"\"\"\n",
        "    Reads a TSV file and returns the number of rows (length) in the file.\n",
        "\n",
        "    Args:\n",
        "        tsv_file_path (str): Path to the TSV file.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of rows in the TSV file.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(tsv_file_path, sep='\\t', header=None)  # Read TSV file into a DataFrame\n",
        "    return len(df)  # Return the number of rows\n",
        "\n",
        "# Example usage:\n",
        "tsv_file_path = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train.tsv'\n",
        "length = get_tsv_length(tsv_file_path)\n",
        "print(f\"Number of rows in the TSV file: {length}\")"
      ],
      "metadata": {
        "id": "LREyq_Zxgn2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5zCrvEOyS5BH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}