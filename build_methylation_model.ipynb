{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "V28",
      "mount_file_id": "1WHPUB6RcAJkyI9YCOjpXy_q16ifWd-gu",
      "authorship_tag": "ABX9TyMcUO3sB3yIb+siLNmAzV6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quang-m-nguyen/DeepPGD/blob/main/build_methylation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tRf_ZxO9l-k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_methylation_model(input_shape, output_units=1):\n",
        "    \"\"\"\n",
        "    Build a neural network model for predicting DNA methylation status from DNABERT2 embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    input_shape (tuple): Shape of the DNABERT2 embedding input (e.g., (768,) for BERT-base embeddings).\n",
        "    output_units (int): Number of output units. Default is 1 for binary classification (methylated or not).\n",
        "\n",
        "    Returns:\n",
        "    model (tf.keras.Model): Compiled Keras model ready for training.\n",
        "    \"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Input layer for DNABERT2 embeddings\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "\n",
        "    # Dense layer with ReLU activation\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    # Dropout layer for regularization\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Another Dense layer\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "    # Output layer for binary classification (use sigmoid for binary output)\n",
        "    model.add(layers.Dense(output_units, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pickle\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def load_embeddings(file_path):\n",
        "    \"\"\"\n",
        "    Load the precomputed embeddings from a pickle file and log the shape.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): Path to the embeddings pickle file.\n",
        "\n",
        "    Returns:\n",
        "    embeddings (np.array): The embeddings data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        embeddings = pickle.load(f)\n",
        "\n",
        "    # Extract embeddings and convert to a NumPy array\n",
        "    embedding_data = np.array([item['embedding'].detach().numpy()  for item in embeddings])\n",
        "\n",
        "    # Log the shape of the embeddings\n",
        "    logging.info(f\"Loaded embeddings shape: {embedding_data.shape}\")\n",
        "\n",
        "    return embedding_data\n",
        "\n",
        "def map_labels_to_embeddings(df, embeddings):\n",
        "    \"\"\"\n",
        "    Map labels to the corresponding embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame with 'label' and 'text' columns.\n",
        "    embeddings (np.array): Precomputed embeddings.\n",
        "\n",
        "    Returns:\n",
        "    X (np.array): Mapped embeddings.\n",
        "    y (np.array): Corresponding labels.\n",
        "    \"\"\"\n",
        "    # Extract labels\n",
        "    labels = df['label'].values\n",
        "\n",
        "    # Ensure embeddings and labels are aligned\n",
        "    if len(labels) != len(embeddings):\n",
        "        raise ValueError(\"The number of labels and embeddings must be the same.\")\n",
        "\n",
        "    return embeddings, labels\n",
        "\n",
        "# Load the embeddings from the pickle file\n",
        "embeddings_file_path = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/combined_embeddings.pkl'\n",
        "embeddings = load_embeddings(embeddings_file_path)\n",
        "\n",
        "# Load the dataset from a TSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/combined_train_test.tsv', sep='\\t')\n",
        "\n",
        "# Map labels to embeddings\n",
        "X, y = map_labels_to_embeddings(df, embeddings)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Now, X_train, y_train, X_val, y_val are ready for model training"
      ],
      "metadata": {
        "id": "W8RIXyDzSr27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "import time\n",
        "\n",
        "def train_model(X_train, y_train, X_val, y_val, model, batch_size=32, epochs=50):\n",
        "    \"\"\"\n",
        "    Trains the given model using the training data and logs performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "    X_train (np.array): Training embeddings.\n",
        "    y_train (np.array): Training labels.\n",
        "    X_val (np.array): Validation embeddings.\n",
        "    y_val (np.array): Validation labels.\n",
        "    model (tf.keras.Model): The model to train.\n",
        "    batch_size (int): Batch size for training. Default is 32.\n",
        "    epochs (int): Number of epochs to train. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "    history: Training history object.\n",
        "    \"\"\"\n",
        "    # Create directories for logging and saving models\n",
        "    log_dir = \"/content/drive/MyDrive/deepPGD/logs/fit/\" + time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    model_save_path = \"best_model.keras\"\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(filepath=model_save_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
        "        EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
        "        TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=2  # Verbose = 2 for better logging during training\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "model = build_methylation_model(input_shape=(768,))\n",
        "# Train the model and log performance\n",
        "history = train_model(X_train, y_train, X_val, y_val, model, batch_size=500, epochs=50)"
      ],
      "metadata": {
        "id": "uk6vL5X9S1ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def join_tsv_files(file_path1, file_path2):\n",
        "    \"\"\"\n",
        "    Joins two TSV files, keeps only the header from the first file,\n",
        "    and saves the result to a new file named 'combined_tsv'.\n",
        "    Includes validation to check the order and size of the combined file.\n",
        "\n",
        "    Parameters:\n",
        "      file_path1 (str): Path to the first TSV file.\n",
        "      file_path2 (str): Path to the second TSV file.\n",
        "    \"\"\"\n",
        "    df1 = pd.read_csv(file_path1, sep='\\t')  # Read both files with headers\n",
        "    df2 = pd.read_csv(file_path2, sep='\\t')  # Read both files with headers\n",
        "\n",
        "    combined_df = pd.concat([df1, df2], ignore_index=True)  # Combine the dataframes\n",
        "\n",
        "    # Validation: Check the 5th item from the first list and 5th last item from the second list\n",
        "    fifth_item_df1 = df1.iloc[4].values.tolist()  # 5th item from df1\n",
        "    fifth_combined = combined_df.iloc[4].values.tolist()  # 5th item from combined\n",
        "\n",
        "    # Check 5th last item from both df2 and combined_df\n",
        "    fifth_last_df2 = df2.iloc[-5].values.tolist()  # 5th last item from df2\n",
        "    fifth_last_combined = combined_df.iloc[-5].values.tolist()  # 5th last item from combined\n",
        "\n",
        "\n",
        "    # Validate if 5th item matches between df1 and combined\n",
        "    if fifth_item_df1 != fifth_combined:\n",
        "        print(\"Error: Incorrect 5th item in combined file (from df1).\")\n",
        "        return  # Terminate if error\n",
        "\n",
        "    # Validate if the 5th last item from df2 matches the 5th last item from the combined DataFrame\n",
        "    if fifth_last_df2 != fifth_last_combined:\n",
        "        print(\"Error: Incorrect 5th last item in combined file (from df2).\")\n",
        "        return  # Terminate if error\n",
        "\n",
        "    # Additional Validation: Check if the bottom of combined matches df2\n",
        "    bottom_combined = combined_df.tail(len(df2)).values.tolist()\n",
        "    df2_values = df2.values.tolist()\n",
        "\n",
        "    if bottom_combined != df2_values:\n",
        "        print(\"Error: The bottom of the combined file does not match df2.\")\n",
        "        return  # Terminate if error\n",
        "\n",
        "    # Validate the size of the combined dataframe\n",
        "    if len(combined_df) != len(df1) + len(df2):\n",
        "        print(\"Error: Incorrect size of combined file.\")\n",
        "        return  # Terminate if error\n",
        "\n",
        "    # Extract directory from file_path1\n",
        "    directory = os.path.dirname(file_path1)\n",
        "\n",
        "    # Save the combined DataFrame to a new file\n",
        "    combined_file_path = os.path.join(directory, 'combined_train_test.tsv')\n",
        "    combined_df.to_csv(combined_file_path, sep='\\t', index=False)\n",
        "\n",
        "# Example usage\n",
        "file_path1 = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train.tsv'\n",
        "file_path2 = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test.tsv'\n",
        "join_tsv_files(file_path1, file_path2)"
      ],
      "metadata": {
        "id": "TH553FsoUEUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def join_pkl_files(file_path1, file_path2, output_dir=None):\n",
        "    \"\"\"\n",
        "    Joins two pickle files and saves the combined data to a new pickle file\n",
        "    in the specified output directory. If output_dir is not provided,\n",
        "    the directory of the first file is used.\n",
        "\n",
        "    Parameters:\n",
        "        file_path1 (str): Path to the first pickle file.\n",
        "        file_path2 (str): Path to the second pickle file.\n",
        "        output_dir (str, optional): Path to the directory where the combined file will be saved.\n",
        "                                    Defaults to the directory of file_path1.\n",
        "    \"\"\"\n",
        "    # Load data from the first pickle file\n",
        "    with open(file_path1, 'rb') as f1:\n",
        "        data1 = pickle.load(f1)\n",
        "    len1 = len(data1)\n",
        "\n",
        "    # Load data from the second pickle file\n",
        "    with open(file_path2, 'rb') as f2:\n",
        "        data2 = pickle.load(f2)\n",
        "    len2 = len(data2)\n",
        "\n",
        "    # Combine the data\n",
        "    combined_data = data1 + data2\n",
        "\n",
        "    # Validation checks\n",
        "    # Check the 5th item from the first list (data1) and combined data\n",
        "    if len1 >= 5:\n",
        "        fifth_item_data1 = data1[4]\n",
        "        fifth_item_combined = combined_data[4]\n",
        "\n",
        "        if fifth_item_data1 != fifth_item_combined:\n",
        "            print(\"Error: 5th item in combined data does not match the 5th item from the first file.\")\n",
        "            return\n",
        "\n",
        "    # Check the 5th last item from the second list (data2) and combined data\n",
        "    if len2 >= 5:\n",
        "        fifth_last_item_data2 = data2[-5]\n",
        "        fifth_last_item_combined = combined_data[-5]\n",
        "\n",
        "        if fifth_last_item_data2 != fifth_last_item_combined:\n",
        "            print(\"Error: 5th last item in combined data does not match the 5th last item from the second file.\")\n",
        "            return\n",
        "\n",
        "    # Validate if the bottom of the combined data matches the second file (data2)\n",
        "    bottom_combined_data = combined_data[-len2:]\n",
        "    if bottom_combined_data != data2:\n",
        "        print(\"Error: The bottom part of the combined data does not match the second file.\")\n",
        "        return\n",
        "\n",
        "    # Validate the length of the combined data\n",
        "    if len(combined_data) != len1 + len2:\n",
        "        print(\"Error: Length of combined data is incorrect.\")\n",
        "        return\n",
        "\n",
        "    # Set output directory to the directory of file_path1 if not provided\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.dirname(file_path1)\n",
        "\n",
        "    # Check if the output directory exists, and create it if it doesn't\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Save the combined data to a new pickle file after successful validation\n",
        "    combined_file_path = os.path.join(output_dir, 'combined_embeddings.pkl')\n",
        "    with open(combined_file_path, 'wb') as f:\n",
        "        pickle.dump(combined_data, f)\n",
        "\n",
        "    print(f\"Combined file created successfully at {combined_file_path} with correct length.\")\n",
        "\n",
        "# Example usage\n",
        "file_path1 = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/train_embeddings_data.pkl'\n",
        "file_path2 = '/content/drive/MyDrive/deepPGD/4mC/4mC_F.vesca/test_embeddings_data.pkl'\n",
        "join_pkl_files(file_path1, file_path2)"
      ],
      "metadata": {
        "id": "gE0Pt5CmevHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6j7c79Aaeuz6"
      }
    }
  ]
}